## 激活函数

### 为什么使用激活函数

* 非线性：激活函数可以引入非线性因素，解决线性模型所不能解决的问题
* 可微性：梯度优化方法所需
* 单调性：当激活函数是单调的时候，单层网络能够保证是凸函数。
* f(x) 约等于x：如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效
* 输出值的范围被限制

### 常使用的激活函数

* sigmoid 

缺点：当输入非常大或者非常小的时候（saturation），这些神经元的梯度是接近于0的；输出均值非0.

* tanh

逐渐替代sigmoid，完全可微分的，反对称，对称中心在原点

* ReLU

使用这个函数能使计算变得很快，然而，当输入为负值的时候，ReLU 的学习速度可能会变得很慢，甚至使神经元直接无效，因为此时输入小于零而梯度为零，从而其权重无法得到更新，在剩下的训练过程中会一直保持静默。

* Leaky ReLUs

带泄露修正线性单元（Leaky ReLU）的输出对负值输入有很小的坡度。由于导数总是不为零，这能减少静默神经元的出现，允许基于梯度的学习

* RReLU

为负值输入添加了一个线性项。而最关键的区别是，这个线性项的斜率在每一个节点上都是随机分配的（通常服从均匀分布）

### 参考链接

[激活函数可视化](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)

[上文的中文链接](https://www.jiqizhixin.com/articles/2017-10-10-3)